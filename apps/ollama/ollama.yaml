apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: apps
  labels:
    app: ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      initContainers:
        - name: pull-models
          image: ollama/ollama:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: OLLAMA_MODELS
              value: /root/.ollama/models
            - name: OLLAMA_HOST
              value: http://127.0.0.1:11434
          command:
            - sh
            - -lc
            - |
              set -eu

              MODELS="qwen2.5:7b-instruct"

              echo "Starting temporary ollama server to pull models"
              ollama serve >/tmp/ollama.log 2>&1 &
              PID=$!

              # Wait up to 60s for server to come up (no wget/curl dependency)
              for i in $(seq 1 60); do
                if ollama list >/dev/null 2>&1; then
                  echo "Ollama is ready"
                  break
                fi
                sleep 1
              done

              for MODEL in $MODELS; do
                # BusyBox-safe marker filename
                SAFE="$(echo "$MODEL" | tr ':/.' '---')"
                MARKER="/root/.ollama/.pulled-${SAFE}"

                if [ -f "$MARKER" ]; then
                  echo "Marker found for $MODEL; skipping"
                  continue
                fi

                echo "Pulling model: $MODEL"
                ollama pull "$MODEL"
                touch "$MARKER"
              done

              echo "Stopping temporary ollama server"
              kill "$PID"
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama

      containers:
        - name: ollama
          image: ollama/ollama:latest
          imagePullPolicy: IfNotPresent

          ports:
            - name: http
              containerPort: 11434

          env:
            # Keep the last-used model warm for a long time
            - name: OLLAMA_KEEP_ALIVE
              value: "168h"

            # Raise server-side default context length
            - name: OLLAMA_CONTEXT_LENGTH
              value: "8192"

            # Keep memory predictable even if you have multiple models available
            - name: OLLAMA_MAX_LOADED_MODELS
              value: "1"

            # Prevent request dogpiles
            - name: OLLAMA_MAX_QUEUE
              value: "8"

            # Persist models
            - name: OLLAMA_MODELS
              value: /root/.ollama/models

          lifecycle:
            postStart:
              exec:
                command:
                  - sh
                  - -lc
                  - |
                    # Pre-warm the default model so first real request isn't cold
                    ollama run qwen2.5:7b-instruct "DecÃ­: listo" >/dev/null 2>&1 || true

          resources:
            requests:
              cpu: "250m"
              memory: "4Gi"
            limits:
              memory: "14Gi"

          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama

          readinessProbe:
            httpGet:
              path: /
              port: 11434
            initialDelaySeconds: 5
            periodSeconds: 10

          livenessProbe:
            httpGet:
              path: /
              port: 11434
            initialDelaySeconds: 20
            periodSeconds: 20

      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-pvc
  namespace: apps
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 25Gi
  storageClassName: longhorn
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: apps
spec:
  selector:
    app: ollama
  ports:
    - name: http
      port: 11434
      targetPort: 11434
  type: ClusterIP
